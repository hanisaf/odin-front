{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import optparse\n",
    "from dateutil import parser\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3 as lite\n",
    "import time\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stanford_ne(text):\n",
    "    d = defaultdict(set)\n",
    "    netagged_words = st.tag(nltk.word_tokenize(text))\n",
    "    for tag, chunk in groupby(netagged_words, lambda x:x[1]):\n",
    "        if tag != \"O\":\n",
    "            chk =  \" \".join(w for w, t in chunk)\n",
    "            d[tag].add(chk)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jsonify_entities(entities):\n",
    "    try:\n",
    "        all_e = reduce(lambda x, y: x.union(y), entities.values())\n",
    "    except TypeError:\n",
    "        all_e = []\n",
    "    entities['ALL'] = all_e\n",
    "    return {k:list(v) for k, v in entities.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_for_resources(filename, directory):\n",
    "    msg = 'cannot find resource file ' + filename\n",
    "    if not os.path.exists(filename):\n",
    "        filename = os.path.expanduser(directory + filename)\n",
    "        if not os.path.exists(filename):\n",
    "            logging.error(msg)\n",
    "            logging.info('You need Stanford NER for entity extraction')\n",
    "            logging.info('Download from http://nlp.stanford.edu/software/CRF-NER.shtml')\n",
    "            logging.info('and extract in your home directory/stanford-ner')\n",
    "            exit(-1)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_entities(df):\n",
    "    global st\n",
    "    engine = test_for_resources('stanford-ner.jar', '~/stanford-ner/')\n",
    "    classifier = test_for_resources('english.muc.7class.distsim.crf.ser.gz', '~/stanford-ner/classifiers/')\n",
    "    st = StanfordNERTagger(classifier, engine)\n",
    "    nes = []\n",
    "    i = 0\n",
    "    for t in df.text:\n",
    "        ne = stanford_ne(t)\n",
    "        ne2= jsonify_entities(ne)\n",
    "        nes.append(ne2)\n",
    "        i=i+1\n",
    "        if i % 10 == 0:\n",
    "            logging.log\n",
    "            logging.info(\"%d documents processed\" % i) \n",
    "    df['ENTITIES'] = nes    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readfile(options):\n",
    "    df = None\n",
    "    filename=options.input\n",
    "    if '.csv' in filename:\n",
    "        df=pd.read_csv(filename)\n",
    "    elif '.json' in filename:\n",
    "        df=pd.read_json(filename)\n",
    "    elif '.db' in filename:\n",
    "        df=pd.read_sql(options.query, lite.connect(filename))\n",
    "    else:\n",
    "        logging.error('Unknown file type')\n",
    "        exit(-1)\n",
    "    if not 'text' in df.columns and options.entities=='yes':\n",
    "        logging.error('Text column is required when entities are selected')\n",
    "        exit(-1)\n",
    "    try:\n",
    "        if 'timestamp' in df.columns:\n",
    "            ts = df.timestamp.map(parser.parse)\n",
    "    except:\n",
    "        logging.warning(\"WARNING : cannot parse timestamp, time-date filter may not work\")\n",
    "    return df\n",
    "    #return df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trim(tags):\n",
    "    try:\n",
    "        tags = tags.split(',')\n",
    "        ntags = [x.strip() for x in tags]\n",
    "        if ntags == [\"\"]:\n",
    "            ntags=[]\n",
    "        return ntags\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(options):\n",
    "    logging.info(\"Reading input file ... \")\n",
    "    df = readfile(options)\n",
    "    #df = df.head()\n",
    "    logging.info(\"Preprocessing ...\") \n",
    "    #df = df.fillna(\"\")\n",
    "    df.columns = map(lambda x: x.replace('.', '__'), df.columns)\n",
    "    #del df['index']\n",
    "    #df['timestamp'] = pd.to_datetime(df['timestamp']).apply(lambda x: str(x.to_datetime64()))\n",
    "    cols = set(df.columns) - set(['text', 'timestamp'])\n",
    "    for col in cols:\n",
    "        if col[-1] == 's': #a  text tag column\n",
    "            if type(df[col].ix[0])==type(df['text'].ix[0]):\n",
    "                df[col] = df[col].map(lambda x: trim(x))\n",
    "                \n",
    "    if options.entities=='yes':\n",
    "        logging.info(\"Extracting name entities (slowww) ...\")\n",
    "        df=generate_entities(df)\n",
    "        \n",
    "    logging.info(\"Writing output file ... \")    \n",
    "    if 'id' in df.columns:\n",
    "        df.index = df['id']\n",
    "        \n",
    "    df.to_json(options.output, orient=\"records\")\n",
    "    logging.info(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    usage = \"usage: %prog [options]\"\n",
    "    parser = optparse.OptionParser(usage)\n",
    "    parser.add_option(\"-i\", \"--input\",  dest=\"input\",\n",
    "                    action=\"store\", type=\"string\",\n",
    "                    help=\"\"\"input file name. \n",
    "                    The program takes .csv, .json & .db sqlite files.\n",
    "                    If .db file is used, you need to pass a query\n",
    "                    The program assumes unicode encoding in text files\n",
    "                    \n",
    "                    Column names:\n",
    "                    text: main text to be processed (required when entities are selected)\n",
    "                    \n",
    "                    timestamp (optional): will be used for time-date filter\n",
    "                    \n",
    "                    type (optional): will be used for type selection\n",
    "                    \n",
    "                    id (optional): will be used as document id, must be unique\n",
    "                    \n",
    "                    Singular noun named columns are field columns\n",
    "                    plural named columns (ending with s) are tag columns \n",
    "                     tags will be generated by tokenizing on comma\n",
    "                    \"\"\")\n",
    "\n",
    "    parser.add_option(\"-o\", \"--output\", dest=\"output\",\n",
    "                     action=\"store\", type=\"string\", default=\"out.json\",\n",
    "                     help=\"output file name default is out.json\")\n",
    "    \n",
    "    parser.add_option(\"-e\", \"--generate-entities\", dest=\"entities\",\n",
    "                    action=\"store\", type=\"string\", default=\"no\",\n",
    "                    help=\"\"\"Generate name entities from text yes/no?\n",
    "                      Default is no\"\"\")\n",
    "  \n",
    "    parser.add_option(\"-q\", \"--query\", dest=\"query\",\n",
    "                    action=\"store\", type=\"string\", default=\"select * from documents\",\n",
    "                    help=\"\"\"query to be used for sqlite file, default is\n",
    "                    \"select * from documents\" Do not forget to quote the query\"\"\")    \n",
    "    \n",
    "    options, args = parser.parse_args()\n",
    "\n",
    "    if not options.input:\n",
    "        logging.info(parser.format_help())\n",
    "        return\n",
    "    \n",
    "    if os.path.exists(options.output):\n",
    "        logging.error(\"output file already exist\")\n",
    "        return\n",
    "    \n",
    "    process(options)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()  \n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook preprocessor.ipynb to script\n",
      "[NbConvertApp] Writing 6472 bytes to preprocessor.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script preprocessor.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
